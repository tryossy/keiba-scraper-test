# 年次並列スクレイピングワークフロー

## 概要

`scrape-yearly-parallel.yml` は、**1年分（12ヶ月）のデータを最大6ジョブ並列**で取得するワークフローです。

## 実行イメージ

```
2024年の場合（12ヶ月 → 12ジョブ、最大6並列）

[並列実行 グループ1]  ← 6ジョブ同時
  2024-01
  2024-02
  2024-03
  2024-04
  2024-05
  2024-06

[並列実行 グループ2]  ← 残り6ジョブ同時
  2024-07
  2024-08
  2024-09
  2024-10
  2024-11
  2024-12

[結合処理]
  全12ヶ月のデータを1つのCSVに結合
```

---

## ワークフロー構成

### 3つのジョブ

```
ジョブ1: generate-matrix
  └─ 12ヶ月のマトリックスを生成

ジョブ2: scrape（並列実行）
  ├─ 2024-01 ┐
  ├─ 2024-02 │
  ├─ 2024-03 │
  ├─ 2024-04 ├─ 最大6ジョブ同時実行
  ├─ 2024-05 │
  ├─ 2024-06 ┘
  ├─ 2024-07 ┐
  ├─ 2024-08 │
  ├─ 2024-09 ├─ 次の6ジョブ
  ├─ 2024-10 │
  ├─ 2024-11 │
  └─ 2024-12 ┘

ジョブ3: combine
  └─ 全12ヶ月のデータを結合 + 統計情報生成
```

---

## 使い方

### 1年分のデータを取得

1. GitHub リポジトリ → **Actions**
2. 左メニューから **"Yearly Horse Racing Data Scrape (12 Months Parallel)"** を選択
3. 右側の **"Run workflow"** をクリック
4. 入力欄に設定:
   - **year**: `2024`（取得したい年）
   - **start_month**: `1`（開始月、デフォルト: 1）
   - **end_month**: `12`（終了月、デフォルト: 12）
5. **"Run workflow"** をクリック

---

### 特定期間のみ取得（例: 2024年4月〜6月）

1. 同じ手順で以下を入力:
   - **year**: `2024`
   - **start_month**: `4`
   - **end_month**: `6`
2. **"Run workflow"** をクリック

**結果**: 2024-04、2024-05、2024-06 の3ヶ月のみ取得（3ジョブ並列）

---

## 実行時間の見積もり

### 月単位のスクレイピング時間

| データ量 | 1ヶ月の処理時間 |
|---------|--------------|
| 通常月（100レース） | 30-60分 |
| 多い月（200レース） | 60-120分 |
| 少ない月（50レース） | 15-30分 |

### 1年分の処理時間（並列実行）

**順次実行の場合**:
- 12ヶ月 × 60分 = 720分（12時間）← タイムアウト！

**6並列実行の場合**:
- グループ1: 6ヶ月 × 60分 = 60分（最大値）
- グループ2: 6ヶ月 × 60分 = 60分（最大値）
- **合計: 約2時間**（タイムアウト回避）

---

## 出力ファイル

### Google Drive の保存構造

```
keiba-data/
└── 2024/
    ├── race_data_2024-01.csv       # 1月のデータ
    ├── race_data_2024-02.csv       # 2月のデータ
    ├── race_data_2024-03.csv       # 3月のデータ
    ├── ...
    ├── race_data_2024-12.csv       # 12月のデータ
    ├── race_data_2024_combined.csv # 全月結合データ ★
    └── stats_2024.json             # 統計情報 ★
```

---

### 統計情報（stats_2024.json）

```json
{
  "year": "2024",
  "total_races": 12345,
  "total_columns": 50,
  "file_size_mb": 123.45,
  "monthly_counts": {
    "2024-01": 1050,
    "2024-02": 980,
    "2024-03": 1100,
    ...
  },
  "columns": ["race_id", "date", "place", ...]
}
```

---

## メリット

### 1. タイムアウト回避

**問題**: 1年分を順次実行すると12時間 → GitHub Actions の6時間制限を超える

**解決**: 6並列実行で約2時間に短縮

---

### 2. 実行時間の大幅短縮

| 実行方式 | 実行時間 | 削減率 |
|---------|---------|--------|
| 順次実行 | 12時間 | - |
| 6並列実行 | 2時間 | **83%削減** |

---

### 3. 失敗時の部分再実行

**問題**: 2024-07 のスクレイピングが失敗した場合、全てやり直し？

**解決**: 失敗した月のみ再実行可能

**再実行方法**:
1. GitHub Actions → 該当のワークフロー実行を開く
2. **"Re-run failed jobs"** をクリック
3. 失敗した月のみ再実行される

---

### 4. 柔軟な期間指定

**例1**: 過去1年分
```yaml
year: 2023
start_month: 1
end_month: 12
```

**例2**: 特定の四半期のみ
```yaml
year: 2024
start_month: 10  # Q4のみ
end_month: 12
```

**例3**: 年をまたぐ期間（2回実行）
```yaml
# 1回目: 2023年10-12月
year: 2023
start_month: 10
end_month: 12

# 2回目: 2024年1-3月
year: 2024
start_month: 1
end_month: 3
```

---

## 並列実行数の調整

### サーバー負荷を考慮

デフォルト: **最大6ジョブ並列**

```yaml
strategy:
  max-parallel: 6  # 変更可能
```

**推奨値**:

| max-parallel | 実行時間 | サーバー負荷 | 推奨用途 |
|-------------|---------|------------|---------|
| 2 | 6時間 | 低 | サーバー負荷を最小限に |
| 4 | 3時間 | 中 | バランス型 |
| **6** | **2時間** | **中** | **推奨（デフォルト）** |
| 12 | 1時間 | 高 | 最速だがサーバー負荷大 |

**注意**: `max-parallel` を大きくしすぎると、netkeiba.com のサーバーに負荷をかけます。**6以下を推奨**します。

---

## ワークフロー比較

### 3つのワークフローの使い分け

| ワークフロー | 用途 | ジョブ数 | 並列数 | 実行時間 |
|------------|------|---------|-------|---------|
| **scrape-monthly.yml** | 月次定期実行 | 1 | 1 | 30-60分 |
| **scrape-monthly-parallel.yml** | 1ヶ月を週分割 | 4-5 | 4 | 15-30分 |
| **scrape-yearly-parallel.yml** | 年間データ取得 | 12 | 6 | 2時間 |

### 使い分けの基準

**月次定期実行（毎月自動）**:
→ `scrape-monthly.yml`

**過去1年分を一括取得**:
→ `scrape-yearly-parallel.yml`

**大量データを高速取得**:
→ `scrape-monthly-parallel.yml`（週分割）または `scrape-yearly-parallel.yml`

---

## 実行例

### 例1: 2024年の全データを取得

**入力**:
```
year: 2024
start_month: 1
end_month: 12
```

**実行**:
- 12ジョブが生成される
- 最大6ジョブが同時実行
- グループ1（1-6月）完了後、グループ2（7-12月）実行
- 約2時間で完了

**出力**:
- `race_data_2024-01.csv` 〜 `race_data_2024-12.csv`
- `race_data_2024_combined.csv`（全月結合）
- `stats_2024.json`（統計情報）

---

### 例2: 2023年の後半のみ取得

**入力**:
```
year: 2023
start_month: 7
end_month: 12
```

**実行**:
- 6ジョブが生成される
- 全て同時実行（max-parallel: 6）
- 約1時間で完了

**出力**:
- `race_data_2023-07.csv` 〜 `race_data_2023-12.csv`
- `race_data_2023_combined.csv`（7-12月のみ結合）
- `stats_2023.json`

---

### 例3: 複数年のデータを取得

**方法**: ワークフローを複数回実行

**1回目**:
```
year: 2022
start_month: 1
end_month: 12
```

**2回目**:
```
year: 2023
start_month: 1
end_month: 12
```

**3回目**:
```
year: 2024
start_month: 1
end_month: 12
```

**結果**: Google Drive に3年分のデータが保存される
```
keiba-data/
├── 2022/
│   └── race_data_2022_combined.csv
├── 2023/
│   └── race_data_2023_combined.csv
└── 2024/
    └── race_data_2024_combined.csv
```

---

## トラブルシューティング

### エラー1: タイムアウト（6時間超過）

**原因**: 1ヶ月のスクレイピングに6時間以上かかる

**解決策**:
1. `scrape-monthly-parallel.yml`（週分割）を使用
2. または、`max-parallel` を増やす（最大12）

---

### エラー2: 一部の月が失敗

**症状**: 2024-05 のジョブだけ失敗

**原因**: ネットワークエラー、レースがない、など

**解決策**:
1. 失敗したジョブのログを確認
2. **"Re-run failed jobs"** で該当月のみ再実行
3. データがない月は無視してOK

---

### エラー3: combine ジョブが失敗

**原因**: Google Drive からのダウンロードに失敗

**解決策**:
1. RCLONE_CONFIG が正しく設定されているか確認
2. Google Drive の認証トークンが有効か確認
3. scrape ジョブが成功しているか確認

---

### エラー4: 並列実行数が多すぎてサーバーエラー

**症状**: 複数のジョブで HTTP 503 エラー

**原因**: max-parallel が大きすぎてサーバー負荷超過

**解決策**:
```yaml
max-parallel: 4  # 6 → 4 に減らす
```

---

## カスタマイズ例

### 並列実行数を変更

```yaml
strategy:
  max-parallel: 4  # 6 → 4 に変更（サーバー負荷を抑える）
```

### レート制限を変更（非推奨）

```yaml
- name: データスクレイピング
  run: |
    python scrape_monthly.py \
      --year-month ${{ matrix.month.year_month }} \
      --interval 2.0  # 1.5秒 → 2.0秒に変更
```

### 保存先を変更

```yaml
- name: Google Drive にアップロード
  run: |
    # デフォルト: gdrive:keiba-data/2024/
    # 変更例: gdrive:horse-racing/2024/
    rclone copy "$OUTPUT_FILE" gdrive:horse-racing/${{ matrix.month.year }}/
```

---

## まとめ

### scrape-yearly-parallel.yml の特徴

| 項目 | 詳細 |
|-----|------|
| **対象期間** | 1年分（12ヶ月） |
| **ジョブ数** | 12ジョブ（月ごと） |
| **並列実行** | 最大6ジョブ同時 |
| **実行時間** | 約2時間 |
| **タイムアウト対策** | ✓ 並列実行で短縮 |
| **部分再実行** | ✓ 失敗月のみ |
| **データ結合** | ✓ 自動（combine ジョブ） |
| **統計情報** | ✓ JSON形式で保存 |

### 推奨用途

**このワークフローを使うべき場合**:
- ✓ 過去1年分のデータを一括取得したい
- ✓ 複数年のデータを効率的に取得したい
- ✓ タイムアウトを回避したい
- ✓ 失敗時に部分的に再実行したい

**他のワークフローを使うべき場合**:
- 月次定期実行 → `scrape-monthly.yml`
- 1ヶ月を高速取得 → `scrape-monthly-parallel.yml`

---

## 実行前のチェックリスト

- [ ] GitHub Secrets に `RCLONE_CONFIG` を設定済み
- [ ] Google Drive の認証が有効
- [ ] 取得したい年と期間を確認
- [ ] サーバー負荷を考慮（max-parallel ≤ 6）
- [ ] 十分な時間的余裕がある（約2時間）

---

## 参考資料

- 通常の月次ワークフロー: [README.md](README.md)
- 週分割ワークフロー: [PARALLEL_WORKFLOW.md](PARALLEL_WORKFLOW.md)
- セットアップガイド: [SETUP.md](SETUP.md)
